#En dockerhub todo 3.1 pero en las diapos 3
version: "3"
services:
  zookeeper:
    image: zookeeper:3.6.2 #probamos 3.6.2 no fufa, tampoco con latest
    container_name: zookeeper
    ports:
      - "2181:2181"
  kafka:
    image: wurstmeister/kafka:2.12-2.3.0
    container_name: kafka
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    volumes:
      - ../practica_big_data_2019:/practica_big_data_2019
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "flight_delay_classification_request:1:1"
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
  mongo:
 #con esto desplegamos el contenedor mongo
    image: mongo:3.4 #volver a ver si con 4.4 funciona
    container_name: mongo
    command: --nojournal
    ports:
      - "27017:27017"
#pero tenemos que crear un mongo seed para importar la bbdd
  mongo-seed:
#aunque esta seed no se si solo vale para mongo:3.4
    image: fvilers/mongo-seed:1.0.0
    environment:
      - MONGO_HOST=mongo
      - MONGO_PORT=27017
    volumes:
      - ../practica_big_data_2019:/practica_big_data_2019 
    depends_on:
      - mongo
    command:
#aqui importar nuestra bbdd
      "mongoimport --host mongo --port 27017 --db agile_data_science --mode upsert --collection origin_dest_distances --type json --file /practica_big_data_2019/data/origin_dest_distances.jsonl"
  spark-master:
    build:
      context: ./spark-master
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - "SPARK_LOCAL_IP=spark-master" #INIT_DAEMON_STEP=setup_spark
      - "constraint:node==spark-master"
    depends_on: 
      - kafka
  spark-worker-1:
    build:
      context: ./spark-worker1
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-worker-1"
    volumes:
      - ../practica_big_data_2019:/practica_big_data_2019
  web:
    build: .
    ports:
      - "5000:5000" 
    environment:
      - "PROJECT_HOME=../practica_big_data_2019"
    volumes:
      - ../practica_big_data_2019:/practica_big_data_2019
#mapeamos los volumenes con las carpetas que tienen los ficheros que necesitamos
volumes:
  practica_big_data_2019:

